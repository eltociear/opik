{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Opik with Langchain\n",
    "\n",
    "For this guide, we will be performing a text to sql query generation task using LangChain. We will be using the Chinook database which contains the SQLite database of a music store with both employee, customer and invoice data.\n",
    "\n",
    "We will highlight three different parts of the workflow:\n",
    "\n",
    "1. Creating a synthetic dataset of questions\n",
    "2. Creating a LangChain chain to generate SQL queries\n",
    "3. Automating the evaluation of the SQL queries on the synthetic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an account on Comet.com\n",
    "\n",
    "[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain) and grab you API Key.\n",
    "\n",
    "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet opik langchain langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured, you can check the settings by viewing the config file at /Users/jacquesverre/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import opik\n",
    "\n",
    "opik.configure(use_local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our environment\n",
    "\n",
    "First, we will download the Chinook database and set up our different API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the relevant data\n",
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\"\n",
    "filename = \"./data/chinook/Chinook_Sqlite.sqlite\"\n",
    "\n",
    "folder = os.path.dirname(filename)\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Chinook database downloaded\")\n",
    "    \n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a synthetic dataset\n",
    "\n",
    "In order to create our synthetic dataset, we will be using the OpenAI API to generate 20 different questions that a user might ask based on the Chinook database.\n",
    "\n",
    "In order to ensure that the OpenAI API calls are being tracked, we will be using the `track_openai` function from the `opik` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": [\n",
      "    \"What is the total revenue generated by each genre in the Chinook database?\",\n",
      "    \"What are the top 5 best selling tracks of all time?\",\n",
      "    \"Which customer has spent the most money at the store?\",\n",
      "    \"How many unique albums have been sold in each country?\",\n",
      "    \"What is the average number of tracks in each playlist?\",\n",
      "    \"Which employee has sold the most tracks in the past year?\",\n",
      "    \"Which artist has the highest average track rating?\",\n",
      "    \"What is the distribution of customers by country?\",\n",
      "    \"Which genre has the highest average track duration?\",\n",
      "    \"What is the average revenue per customer for each country?\",\n",
      "    \"How many tracks have been purchased on each day of the week?\",\n",
      "    \"Which track has been played the most number of times?\",\n",
      "    \"What is the average revenue per track for each genre?\",\n",
      "    \"What is the average revenue per customer for each employee?\",\n",
      "    \"Which customer has the highest average track rating?\",\n",
      "    \"Which artist has the highest total revenue from track sales?\",\n",
      "    \"What is the average track rating for each genre?\",\n",
      "    \"What is the average revenue per customer from each genre?\",\n",
      "    \"Which employee has the highest average track rating?\",\n",
      "    \"How does the number of tracks purchased vary by year?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from opik.integrations.openai import track_openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "os.environ[\"OPIK_PROJECT_NAME\"] = \"langchain-integration-demo\"\n",
    "client = OpenAI()\n",
    "\n",
    "openai_client = track_openai(client)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Create 20 different example questions a user might ask based on the Chinook Database.\n",
    "\n",
    "These questions should be complex and require the model to think. They should include complex joins and window functions to answer.\n",
    "\n",
    "Return the response as a json object with a \"result\" key and an array of strings with the question.\n",
    "\"\"\"\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our synthetic dataset, we can create a dataset in Comet and insert the questions into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    }
   ],
   "source": [
    "# Create the synthetic dataset\n",
    "import opik\n",
    "from opik import DatasetItem\n",
    "\n",
    "synthetic_questions = json.loads(completion.choices[0].message.content)[\"result\"]\n",
    "\n",
    "client = opik.Opik()\n",
    "try:\n",
    "    dataset = client.create_dataset(name=\"synthetic_questions\")\n",
    "    dataset.insert([\n",
    "        DatasetItem(input={\"question\": question}) for question in synthetic_questions\n",
    "    ])\n",
    "except opik.rest_api.core.ApiError as e:\n",
    "    print(\"Dataset already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a LangChain chain\n",
    "\n",
    "We will be using the `create_sql_query_chain` function from the `langchain` library to create a SQL query to answer the question.\n",
    "\n",
    "We will be using the `OpikTracer` class from the `opik` library to ensure that the LangChan trace are being tracked in Comet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(\"EmployeeId\") AS \"TotalEmployees\" FROM \"Employee\"\n"
     ]
    }
   ],
   "source": [
    "# Use langchain to create a SQL query to answer the question\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "opik_tracer = OpikTracer(tags=[\"simple_chain\"])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = create_sql_query_chain(llm, db).with_config({\"callbacks\": [opik_tracer]})\n",
    "response = chain.invoke({\"question\": \"How many employees are there ?\"})\n",
    "response\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatting the evaluation\n",
    "\n",
    "In order to ensure our LLM application is working correctly, we will test it on our synthetic dataset.\n",
    "\n",
    "For this we will be using the `evaluate` function from the `opik` library. We will evaluate the application using a custom metric that checks if the SQL query is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 20/20 [00:02<00:00,  7.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─ synthetic_questions (20 samples) ─╮\n",
       "│                                    │\n",
       "│ <span style=\"font-weight: bold\">Total time:       </span> 00:00:03        │\n",
       "│ <span style=\"font-weight: bold\">Number of samples:</span> 20              │\n",
       "│                                    │\n",
       "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">valid_sql_query: 0.8500 (avg)</span>      │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─ synthetic_questions (20 samples) ─╮\n",
       "│                                    │\n",
       "│ \u001b[1mTotal time:       \u001b[0m 00:00:03        │\n",
       "│ \u001b[1mNumber of samples:\u001b[0m 20              │\n",
       "│                                    │\n",
       "│ \u001b[1;32mvalid_sql_query: 0.8500 (avg)\u001b[0m      │\n",
       "│                                    │\n",
       "╰────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Uploading results to Opik <span style=\"color: #808000; text-decoration-color: #808000\">...</span> \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Uploading results to Opik \u001b[33m...\u001b[0m \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'metadata' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m\n\u001b[1;32m     40\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm_chain(item\u001b[38;5;241m.\u001b[39minput[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\n\u001b[1;32m     45\u001b[0m     }\n\u001b[0;32m---> 47\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSQL question answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvalid_sql_query\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages/opik/evaluation/evaluator.py:68\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, task, scoring_metrics, experiment_name, experiment_config, verbose, task_threads)\u001b[0m\n\u001b[1;32m     64\u001b[0m     report\u001b[38;5;241m.\u001b[39mdisplay_experiment_results(dataset\u001b[38;5;241m.\u001b[39mname, total_time, test_results)\n\u001b[1;32m     66\u001b[0m scores_logger\u001b[38;5;241m.\u001b[39mlog_scores(client\u001b[38;5;241m=\u001b[39mclient, test_results\u001b[38;5;241m=\u001b[39mtest_results)\n\u001b[0;32m---> 68\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m experiment_items \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     74\u001b[0m     experiment_item\u001b[38;5;241m.\u001b[39mExperimentItem(\n\u001b[1;32m     75\u001b[0m         dataset_item_id\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mtest_case\u001b[38;5;241m.\u001b[39mdataset_item_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m test_results\n\u001b[1;32m     79\u001b[0m ]\n\u001b[1;32m     81\u001b[0m experiment\u001b[38;5;241m.\u001b[39minsert(experiment_items\u001b[38;5;241m=\u001b[39mexperiment_items)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages/opik/api_objects/opik_client.py:377\u001b[0m, in \u001b[0;36mOpik.create_experiment\u001b[0;34m(self, name, dataset_name, experiment_config)\u001b[0m\n\u001b[1;32m    367\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment config must be dictionary, but \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m was provided. Config will not be logged.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    369\u001b[0m         experiment_config,\n\u001b[1;32m    370\u001b[0m     )\n\u001b[1;32m    371\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest_client\u001b[38;5;241m.\u001b[39mexperiments\u001b[38;5;241m.\u001b[39mcreate_experiment(\n\u001b[1;32m    374\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    375\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m--> 377\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m\u001b[43mmetadata\u001b[49m,\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    380\u001b[0m experiment_ \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mExperiment(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    382\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    383\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m    384\u001b[0m     rest_client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest_client,\n\u001b[1;32m    385\u001b[0m )\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m experiment_\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'metadata' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from opik import Opik, track\n",
    "from opik.evaluation import evaluate\n",
    "from opik.evaluation.metrics import base_metric, score_result\n",
    "from typing import Any\n",
    "\n",
    "class ValidSQLQuery(base_metric.BaseMetric):\n",
    "    def __init__(self, name: str, db: Any):\n",
    "        self.name = name\n",
    "        self.db = db\n",
    "\n",
    "    def score(self, output: str, **ignored_kwargs: Any):\n",
    "        # Add you logic here\n",
    "\n",
    "        try:\n",
    "            db.run(output)\n",
    "            return score_result.ScoreResult(\n",
    "                name=self.name,\n",
    "                value=1,\n",
    "                reason=\"Query ran successfully\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return score_result.ScoreResult(\n",
    "                name=self.name,\n",
    "                value=0,\n",
    "                reason=str(e)\n",
    "            )\n",
    "\n",
    "valid_sql_query = ValidSQLQuery(name=\"valid_sql_query\", db=db)\n",
    "\n",
    "client = Opik()\n",
    "dataset = client.get_dataset(\"synthetic_questions\")\n",
    "\n",
    "@track()\n",
    "def llm_chain(input: str) -> str:\n",
    "    response = chain.invoke({\"question\": input})\n",
    "    \n",
    "    return response\n",
    "\n",
    "def evaluation_task(item):\n",
    "    response = llm_chain(item.input[\"question\"])\n",
    "\n",
    "    return {\n",
    "        \"reference\": \"hello\",\n",
    "        \"output\": response\n",
    "    }\n",
    "\n",
    "res = evaluate(\n",
    "    experiment_name=\"SQL question answering\",\n",
    "    dataset=dataset,\n",
    "    task=evaluation_task,\n",
    "    scoring_metrics=[valid_sql_query]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation results are now uploaded to the Opik platform and can be viewed in the UI.\n",
    "\n",
    "![LangChain Evaluation](https://raw.githubusercontent.com/comet-ml/opik/main/apps/opik-documentation/documentation/static/img/cookbook/langchain_cookbook.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_opik",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
